{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import os.path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Union, Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import os\n",
    "import glob\n",
    "from os.path import normpath, basename\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def check_mkdir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        \n",
    "def create_frames_from_video(video_location, save_folder ,name_prefix='img', extension='jpg'):\n",
    "    # Read the video from specified path\n",
    "    cam = cv2.VideoCapture(video_location)\n",
    "    currentframe = 1\n",
    "    while(True):\n",
    "\n",
    "        # reading from frame\n",
    "        ret,frame = cam.read()\n",
    "        if ret:\n",
    "            # if video is still left continue creating images\n",
    "            name= os.path.join(save_folder, f'{name_prefix}_{currentframe:05d}.{extension}')\n",
    "\n",
    "            # writing the extracted images\n",
    "            cv2.imwrite(name, frame)\n",
    "\n",
    "            # increasing counter so that it will\n",
    "            # show how many frames are created\n",
    "            currentframe += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return currentframe\n",
    "\n",
    "def plot_video(rows, cols, frame_list, plot_width, plot_height, title: str):\n",
    "    fig = plt.figure(figsize=(plot_width, plot_height))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(rows, cols),  # creates 2x2 grid of axes\n",
    "                     axes_pad=0.3,  # pad between axes in inch.\n",
    "                     )\n",
    "\n",
    "    for index, (ax, im) in enumerate(zip(grid, frame_list)):\n",
    "        # Iterating over the grid returns the Axes.\n",
    "        ax.imshow(im)\n",
    "        ax.set_title(index)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "    \n",
    "def denormalize(video_tensor):\n",
    "    \"\"\"\n",
    "    Undoes mean/standard deviation normalization, zero to one scaling,\n",
    "    and channel rearrangement for a batch of images.\n",
    "    args:\n",
    "        video_tensor: a (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "    \"\"\"\n",
    "    inverse_normalize = transforms.Normalize(\n",
    "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
    "    )\n",
    "    return (inverse_normalize(video_tensor) * 255.).type(torch.uint8).permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "class ShiftWithChannelTensor:\n",
    "    def __call__(self, data):\n",
    "        return data.permute(1, 0, 2, 3).contiguous()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MobilenetV2 in PyTorch.\n",
    "See the paper \"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" for more details.\n",
    "'''\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(inp, oup, kernel_size=3, stride=stride, padding=(1,1,1), bias=False),\n",
    "        nn.BatchNorm3d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm3d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == (1,1,1) and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv3d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm3d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv3d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm3d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv3d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm3d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv3d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm3d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv3d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm3d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, sample_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1,  16, 1, (1,1,1)],\n",
    "            [6,  24, 2, (2,2,2)],\n",
    "            [6,  32, 3, (2,2,2)],\n",
    "            [6,  64, 4, (2,2,2)],\n",
    "            [6,  96, 3, (1,1,1)],\n",
    "            [6, 160, 3, (2,2,2)],\n",
    "            [6, 320, 1, (1,1,1)],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert sample_size % 16 == 0.\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, (1,2,2))]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else (1,1,1)\n",
    "                self.features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.avg_pool3d(x, x.data.size()[-3:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImglistToTensor(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Converts a list of PIL images in the range [0,255] to a torch.FloatTensor\n",
    "    of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1].\n",
    "    Can be used as first transform for ``VideoFrameDataset``.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(img_list: List[Image.Image]) -> 'torch.Tensor[NUM_IMAGES, CHANNELS, HEIGHT, WIDTH]':\n",
    "        \"\"\"\n",
    "        Converts each PIL image in a list to\n",
    "        a torch Tensor and stacks them into\n",
    "        a single tensor.\n",
    "\n",
    "        Args:\n",
    "            img_list: list of PIL images.\n",
    "        Returns:\n",
    "            tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        # print(type(img_list))\n",
    "        # print(img_list)\n",
    "        # print(np.array(img_list).shape)\n",
    "        return torch.stack([transforms.functional.to_tensor(pic) for pic in img_list])\n",
    "        # return torch.stack([transforms.functional.to_tensor(img_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from typing import List, Union, Tuple, Any\n",
    "\n",
    "\n",
    "class VideoRecord(object):\n",
    "    \"\"\"\n",
    "    Helper class for class VideoFrameDataset. This class\n",
    "    represents a video sample's metadata.\n",
    "\n",
    "    Args:\n",
    "        root_datapath: the system path to the root folder\n",
    "                       of the videos.\n",
    "        row: A list with four or more elements where 1) The first\n",
    "             element is the path to the video sample's frames excluding\n",
    "             the root_datapath prefix 2) The  second element is the starting frame id of the video\n",
    "             3) The third element is the inclusive ending frame id of the video\n",
    "             4) The fourth element is the label index.\n",
    "             5) any following elements are labels in the case of multi-label classification\n",
    "    \"\"\"\n",
    "    def __init__(self, row, root_datapath):\n",
    "        self._data = row\n",
    "        self._path = os.path.join(root_datapath, row[0])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def path(self) -> str:\n",
    "        return self._path\n",
    "\n",
    "    @property\n",
    "    def num_frames(self) -> int:\n",
    "        return self.end_frame - self.start_frame + 1  # +1 because end frame is inclusive\n",
    "    @property\n",
    "    def start_frame(self) -> int:\n",
    "        return int(self._data[1])\n",
    "\n",
    "    @property\n",
    "    def end_frame(self) -> int:\n",
    "        return int(self._data[2])\n",
    "\n",
    "    @property\n",
    "    def label(self) -> Union[int, List[int]]:\n",
    "        # just one label_id\n",
    "        if len(self._data) == 4:\n",
    "            return int(self._data[3])\n",
    "        # sample associated with multiple labels\n",
    "        else:\n",
    "            return [int(label_id) for label_id in self._data[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFrameDataset(torch.utils.data.Dataset):\n",
    "    r\"\"\"\n",
    "    A highly efficient and adaptable dataset class for videos.\n",
    "    Instead of loading every frame of a video,\n",
    "    loads x RGB frames of a video (sparse temporal sampling) and evenly\n",
    "    chooses those frames from start to end of the video, returning\n",
    "    a list of x PIL images or ``FRAMES x CHANNELS x HEIGHT x WIDTH``\n",
    "    tensors where FRAMES=x if the ``ImglistToTensor()``\n",
    "    transform is used.\n",
    "\n",
    "    More specifically, the frame range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS\n",
    "    segments and FRAMES_PER_SEGMENT consecutive frames are taken from each segment.\n",
    "\n",
    "    Note:\n",
    "        A demonstration of using this class can be seen\n",
    "        in ``demo.py``\n",
    "        https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch\n",
    "\n",
    "    Note:\n",
    "        This dataset broadly corresponds to the frame sampling technique\n",
    "        introduced in ``Temporal Segment Networks`` at ECCV2016\n",
    "        https://arxiv.org/abs/1608.00859.\n",
    "\n",
    "\n",
    "    Note:\n",
    "        This class relies on receiving video data in a structure where\n",
    "        inside a ``ROOT_DATA`` folder, each video lies in its own folder,\n",
    "        where each video folder contains the frames of the video as\n",
    "        individual files with a naming convention such as\n",
    "        img_001.jpg ... img_059.jpg.\n",
    "        For enumeration and annotations, this class expects to receive\n",
    "        the path to a .txt file where each video sample has a row with four\n",
    "        (or more in the case of multi-label, see README on Github)\n",
    "        space separated values:\n",
    "        ``VIDEO_FOLDER_PATH     START_FRAME      END_FRAME      LABEL_INDEX``.\n",
    "        ``VIDEO_FOLDER_PATH`` is expected to be the path of a video folder\n",
    "        excluding the ``ROOT_DATA`` prefix. For example, ``ROOT_DATA`` might\n",
    "        be ``home\\data\\datasetxyz\\videos\\``, inside of which a ``VIDEO_FOLDER_PATH``\n",
    "        might be ``jumping\\0052\\`` or ``sample1\\`` or ``00053\\``.\n",
    "\n",
    "    Args:\n",
    "        root_path: The root path in which video folders lie.\n",
    "                   this is ROOT_DATA from the description above.\n",
    "        annotationfile_path: The .txt annotation file containing\n",
    "                             one row per video sample as described above.\n",
    "        num_segments: The number of segments the video should\n",
    "                      be divided into to sample frames from.\n",
    "        frames_per_segment: The number of frames that should\n",
    "                            be loaded per segment. For each segment's\n",
    "                            frame-range, a random start index or the\n",
    "                            center is chosen, from which frames_per_segment\n",
    "                            consecutive frames are loaded.\n",
    "        imagefile_template: The image filename template that video frame files\n",
    "                            have inside of their video folders as described above.\n",
    "        transform: Transform pipeline that receives a list of PIL images/frames.\n",
    "        test_mode: If True, frames are taken from the center of each\n",
    "                   segment, instead of a random location in each segment.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root_path: str,\n",
    "                 annotationfile_path: str,\n",
    "                 num_segments: int = 3,\n",
    "                 frames_per_segment: int = 1,\n",
    "                 imagefile_template: str='img_{:05d}.jpg',\n",
    "                 transform = None,\n",
    "                 test_mode: bool = False):\n",
    "        super(VideoFrameDataset, self).__init__()\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.annotationfile_path = annotationfile_path\n",
    "        self.num_segments = num_segments\n",
    "        self.frames_per_segment = frames_per_segment\n",
    "        self.imagefile_template = imagefile_template\n",
    "        self.transform = transform\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "        self._parse_annotationfile()\n",
    "        self._sanity_check_samples()\n",
    "\n",
    "    def _load_image(self, directory: str, idx: int) -> Image.Image:\n",
    "        return Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert('RGB')\n",
    "\n",
    "    def _parse_annotationfile(self):\n",
    "        self.video_list = [VideoRecord(x.strip().split(), self.root_path) for x in open(self.annotationfile_path)]\n",
    "\n",
    "    def _sanity_check_samples(self):\n",
    "        for record in self.video_list:\n",
    "            if record.num_frames <= 0 or record.start_frame == record.end_frame:\n",
    "                print(f\"\\nDataset Warning: video {record.path} seems to have zero RGB frames on disk!\\n\")\n",
    "\n",
    "            elif record.num_frames < (self.num_segments * self.frames_per_segment):\n",
    "                print(f\"\\nDataset Warning: video {record.path} has {record.num_frames} frames \"\n",
    "                      f\"but the dataloader is set up to load \"\n",
    "                      f\"(num_segments={self.num_segments})*(frames_per_segment={self.frames_per_segment})\"\n",
    "                      f\"={self.num_segments * self.frames_per_segment} frames. Dataloader will throw an \"\n",
    "                      f\"error when trying to load this video.\\n\")\n",
    "\n",
    "    def _get_start_indices(self, record: VideoRecord) -> 'np.ndarray[int]':\n",
    "        \"\"\"\n",
    "        For each segment, choose a start index from where frames\n",
    "        are to be loaded from.\n",
    "\n",
    "        Args:\n",
    "            record: VideoRecord denoting a video sample.\n",
    "        Returns:\n",
    "            List of indices of where the frames of each\n",
    "            segment are to be loaded from.\n",
    "        \"\"\"\n",
    "        # choose start indices that are perfectly evenly spread across the video frames.\n",
    "        if self.test_mode:\n",
    "            distance_between_indices = (record.num_frames - self.frames_per_segment + 1) / float(self.num_segments)\n",
    "\n",
    "            start_indices = np.array([int(distance_between_indices / 2.0 + distance_between_indices * x)\n",
    "                                      for x in range(self.num_segments)])\n",
    "        # randomly sample start indices that are approximately evenly spread across the video frames.\n",
    "        else:\n",
    "            max_valid_start_index = (record.num_frames - self.frames_per_segment + 1) // self.num_segments\n",
    "\n",
    "            start_indices = np.multiply(list(range(self.num_segments)), max_valid_start_index) + \\\n",
    "                      np.random.randint(max_valid_start_index, size=self.num_segments)\n",
    "\n",
    "        return start_indices\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Union[\n",
    "        Tuple[List[Image.Image], Union[int, List[int]]],\n",
    "        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n",
    "        Tuple[Any, Union[int, List[int]]],\n",
    "        ]:\n",
    "        \"\"\"\n",
    "        For video with id idx, loads self.NUM_SEGMENTS * self.FRAMES_PER_SEGMENT\n",
    "        frames from evenly chosen locations across the video.\n",
    "\n",
    "        Args:\n",
    "            idx: Video sample index.\n",
    "        Returns:\n",
    "            A tuple of (video, label). Label is either a single\n",
    "            integer or a list of integers in the case of multiple labels.\n",
    "            Video is either 1) a list of PIL images if no transform is used\n",
    "            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n",
    "            if the transform \"ImglistToTensor\" is used\n",
    "            3) or anything else if a custom transform is used.\n",
    "        \"\"\"\n",
    "        record: VideoRecord = self.video_list[idx]\n",
    "\n",
    "        frame_start_indices: 'np.ndarray[int]' = self._get_start_indices(record)\n",
    "\n",
    "        return self._get(record, frame_start_indices)\n",
    "\n",
    "    def _get(self, record: VideoRecord, frame_start_indices: 'np.ndarray[int]') -> Union[\n",
    "        Tuple[List[Image.Image], Union[int, List[int]]],\n",
    "        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n",
    "        Tuple[Any, Union[int, List[int]]],\n",
    "        ]:\n",
    "        \"\"\"\n",
    "        Loads the frames of a video at the corresponding\n",
    "        indices.\n",
    "\n",
    "        Args:\n",
    "            record: VideoRecord denoting a video sample.\n",
    "            frame_start_indices: Indices from which to load consecutive frames from.\n",
    "        Returns:\n",
    "            A tuple of (video, label). Label is either a single\n",
    "            integer or a list of integers in the case of multiple labels.\n",
    "            Video is either 1) a list of PIL images if no transform is used\n",
    "            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n",
    "            if the transform \"ImglistToTensor\" is used\n",
    "            3) or anything else if a custom transform is used.\n",
    "        \"\"\"\n",
    "\n",
    "        frame_start_indices = frame_start_indices + record.start_frame\n",
    "        images = list()\n",
    "\n",
    "        # from each start_index, load self.frames_per_segment\n",
    "        # consecutive frames\n",
    "        for start_index in frame_start_indices:\n",
    "            frame_index = int(start_index)\n",
    "\n",
    "            # load self.frames_per_segment consecutive frames\n",
    "            for _ in range(self.frames_per_segment):\n",
    "                image = self._load_image(record.path, frame_index)\n",
    "                images.append(image)\n",
    "\n",
    "                if frame_index < record.end_frame:\n",
    "                    frame_index += 1\n",
    "\n",
    "        if self.transform is not None:\n",
    "            images = self.transform(images)\n",
    "\n",
    "        return images, record.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "def eval_preprocess(size):\n",
    "    preprocess = transforms.Compose([\n",
    "        ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "        transforms.Resize(size),  # image batch, resize smaller edge to 299\n",
    "        transforms.CenterCrop(size),  # image batch, center crop to square 299x299\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ShiftWithChannelTensor()\n",
    "    ])\n",
    "    return preprocess\n",
    "\n",
    "def train_preprocess(size):\n",
    "    train_preprocess = transforms.Compose([\n",
    "        ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "        transforms.Resize(size),  # image batch, resize smaller edge to 299\n",
    "        transforms.RandomCrop(size),  # image batch, center crop to square 299x299\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ShiftWithChannelTensor()\n",
    "    ])\n",
    "    return train_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "PATH = 'model_state_dict.pt'\n",
    "device = torch.device('cuda')\n",
    "model = MobileNetV2(num_classes=6, sample_size=112, width_mult=1.0)\n",
    "model.load_state_dict(torch.load(PATH),strict=False)\n",
    "model.to(device)\n",
    "# freezing layer\n",
    "ct = 0\n",
    "for child in model.children():\n",
    "    for param in child.parameters():\n",
    "            ct +=1\n",
    "            if ct <80:\n",
    "                param.requires_grad = False\n",
    "optimizer = optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport glob\\nfrom os.path import normpath, basename\\nfrom sklearn.model_selection import train_test_split\\n\\nDATA_PATH = \\'transfer/activities\\'#dataset\\n# create dictionary of activities\\n\\nlist_activities= os.listdir(DATA_PATH)\\nlist_dict= {}\\nfor index,activity in enumerate(list_activities):\\n    list_dict[activity] = index\\n    \\n# create a note \\nwith open(\\'label_notes.txt\\', \\'w\\') as f:\\n    for key,value in list_dict.items():\\n        f.write(f\"{key}: {value}\")\\n        f.write(\\'\\n\\')\\n\\n# create list of data\\nall_x =[]\\nall_y = []\\nfor path, subdirs, files in os.walk(DATA_PATH):\\n    for name in files:\\n        all_x.append(os.path.join(path, name))\\n        all_y.append(list_dict[basename(normpath(path))])\\n\\nprint(f\"Currently have {len(all_x)} video data...\")  \\n\\n# split to train and test\\nX_train, X_test, y_train, y_test = train_test_split(all_x, all_y, test_size=0.2, random_state=42, stratify=all_y)\\n\\n# generate image for train & test\\ncheck_mkdir(\\'transfer/train\\')#dataset/train\\ncheck_mkdir(\\'transfer/test\\')#dataset/test\\n\\nfor key,value in list_dict.items():\\n    check_mkdir(os.path.join(\\'transfer/train\\',str(value)))\\n    check_mkdir(os.path.join(\\'transfer/test\\',str(value)))\\n\\nwith open(\\'transfer/train/annotations.txt\\', \\'w\\') as f:\\n    for index,video in enumerate(X_train):\\n        vid_in_folder = len(os.listdir(os.path.join(\\'transfer/train\\',str(y_train[index]))))\\n        path_folder = os.path.join(\\'transfer/train\\',str(y_train[index]),str(vid_in_folder+1).zfill(5))\\n        check_mkdir(path_folder)\\n        # parse video into frame\\n        last_frame = create_frames_from_video(video,path_folder)\\n        # create note\\n        f.write(f\\'{y_train[index]}/{str(vid_in_folder+1).zfill(5)} 1 {last_frame-1} {y_train[index]}\\')\\n        f.write(\\'\\n\\')\\n        \\nwith open(\\'transfer/test/annotations.txt\\', \\'w\\') as f:\\n    for index,video in enumerate(X_test):\\n        vid_in_folder = len(os.listdir(os.path.join(\\'transfer/test\\',str(y_test[index]))))\\n        path_folder = os.path.join(\\'transfer/test\\',str(y_test[index]),str(vid_in_folder+1).zfill(5))\\n        check_mkdir(path_folder)\\n        # parse video into frame\\n        last_frame = create_frames_from_video(video,path_folder)\\n        # create note\\n        f.write(f\\'{y_test[index]}/{str(vid_in_folder+1).zfill(5)} 1 {last_frame-1} {y_test[index]}\\')\\n        f.write(\\'\\n\\')'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import glob\n",
    "from os.path import normpath, basename\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_PATH = 'transfer/activities'#dataset\n",
    "# create dictionary of activities\n",
    "\n",
    "list_activities= os.listdir(DATA_PATH)\n",
    "list_dict= {}\n",
    "for index,activity in enumerate(list_activities):\n",
    "    list_dict[activity] = index\n",
    "    \n",
    "# create a note \n",
    "with open('label_notes.txt', 'w') as f:\n",
    "    for key,value in list_dict.items():\n",
    "        f.write(f\"{key}: {value}\")\n",
    "        f.write('\\n')\n",
    "\n",
    "# create list of data\n",
    "all_x =[]\n",
    "all_y = []\n",
    "for path, subdirs, files in os.walk(DATA_PATH):\n",
    "    for name in files:\n",
    "        all_x.append(os.path.join(path, name))\n",
    "        all_y.append(list_dict[basename(normpath(path))])\n",
    "\n",
    "print(f\"Currently have {len(all_x)} video data...\")  \n",
    "\n",
    "# split to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_x, all_y, test_size=0.2, random_state=42, stratify=all_y)\n",
    "\n",
    "# generate image for train & test\n",
    "check_mkdir('transfer/train')#dataset/train\n",
    "check_mkdir('transfer/test')#dataset/test\n",
    "\n",
    "for key,value in list_dict.items():\n",
    "    check_mkdir(os.path.join('transfer/train',str(value)))\n",
    "    check_mkdir(os.path.join('transfer/test',str(value)))\n",
    "\n",
    "with open('transfer/train/annotations.txt', 'w') as f:\n",
    "    for index,video in enumerate(X_train):\n",
    "        vid_in_folder = len(os.listdir(os.path.join('transfer/train',str(y_train[index]))))\n",
    "        path_folder = os.path.join('transfer/train',str(y_train[index]),str(vid_in_folder+1).zfill(5))\n",
    "        check_mkdir(path_folder)\n",
    "        # parse video into frame\n",
    "        last_frame = create_frames_from_video(video,path_folder)\n",
    "        # create note\n",
    "        f.write(f'{y_train[index]}/{str(vid_in_folder+1).zfill(5)} 1 {last_frame-1} {y_train[index]}')\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open('transfer/test/annotations.txt', 'w') as f:\n",
    "    for index,video in enumerate(X_test):\n",
    "        vid_in_folder = len(os.listdir(os.path.join('transfer/test',str(y_test[index]))))\n",
    "        path_folder = os.path.join('transfer/test',str(y_test[index]),str(vid_in_folder+1).zfill(5))\n",
    "        check_mkdir(path_folder)\n",
    "        # parse video into frame\n",
    "        last_frame = create_frames_from_video(video,path_folder)\n",
    "        # create note\n",
    "        f.write(f'{y_test[index]}/{str(vid_in_folder+1).zfill(5)} 1 {last_frame-1} {y_test[index]}')\n",
    "        f.write('\\n')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on gpu\n",
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch17_p38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.263919 \tTraining acc: 0.45 \tValidation Loss: 1.159654 \tAccuracy: 0.47 \tF1-Score: 0.30\n",
      "Validation loss decreased (inf --> 1.159654).  Saving model ...\n",
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch17_p38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 1.143819 \tTraining acc: 0.51 \tValidation Loss: 1.184145 \tAccuracy: 0.47 \tF1-Score: 0.30\n",
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch17_p38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 1.100495 \tTraining acc: 0.54 \tValidation Loss: 1.035363 \tAccuracy: 0.58 \tF1-Score: 0.41\n",
      "Validation loss decreased (1.159654 --> 1.035363).  Saving model ...\n",
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch17_p38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 1.058870 \tTraining acc: 0.55 \tValidation Loss: 1.016200 \tAccuracy: 0.56 \tF1-Score: 0.40\n",
      "Validation loss decreased (1.035363 --> 1.016200).  Saving model ...\n",
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\pytorch17_p38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 1.021350 \tTraining acc: 0.57 \tValidation Loss: 0.987885 \tAccuracy: 0.62 \tF1-Score: 0.48\n",
      "Validation loss decreased (1.016200 --> 0.987885).  Saving model ...\n",
      "validation\n",
      "Epoch: 6 \tTraining Loss: 0.991509 \tTraining acc: 0.57 \tValidation Loss: 0.945806 \tAccuracy: 0.60 \tF1-Score: 0.49\n",
      "Validation loss decreased (0.987885 --> 0.945806).  Saving model ...\n",
      "validation\n",
      "Epoch: 7 \tTraining Loss: 0.958132 \tTraining acc: 0.59 \tValidation Loss: 0.936931 \tAccuracy: 0.59 \tF1-Score: 0.51\n",
      "Validation loss decreased (0.945806 --> 0.936931).  Saving model ...\n",
      "validation\n",
      "Epoch: 8 \tTraining Loss: 0.954316 \tTraining acc: 0.60 \tValidation Loss: 0.871492 \tAccuracy: 0.66 \tF1-Score: 0.63\n",
      "Validation loss decreased (0.936931 --> 0.871492).  Saving model ...\n",
      "validation\n",
      "Epoch: 9 \tTraining Loss: 0.940024 \tTraining acc: 0.60 \tValidation Loss: 0.878105 \tAccuracy: 0.62 \tF1-Score: 0.60\n",
      "validation\n",
      "Epoch: 10 \tTraining Loss: 0.893055 \tTraining acc: 0.63 \tValidation Loss: 0.825641 \tAccuracy: 0.66 \tF1-Score: 0.64\n",
      "Validation loss decreased (0.871492 --> 0.825641).  Saving model ...\n",
      "validation\n",
      "Epoch: 11 \tTraining Loss: 0.910844 \tTraining acc: 0.61 \tValidation Loss: 0.819471 \tAccuracy: 0.65 \tF1-Score: 0.64\n",
      "Validation loss decreased (0.825641 --> 0.819471).  Saving model ...\n",
      "validation\n",
      "Epoch: 12 \tTraining Loss: 0.893216 \tTraining acc: 0.62 \tValidation Loss: 0.873998 \tAccuracy: 0.65 \tF1-Score: 0.64\n",
      "validation\n",
      "Epoch: 13 \tTraining Loss: 0.890452 \tTraining acc: 0.63 \tValidation Loss: 0.804614 \tAccuracy: 0.66 \tF1-Score: 0.68\n",
      "Validation loss decreased (0.819471 --> 0.804614).  Saving model ...\n",
      "validation\n",
      "Epoch: 14 \tTraining Loss: 0.901364 \tTraining acc: 0.63 \tValidation Loss: 0.813826 \tAccuracy: 0.66 \tF1-Score: 0.64\n",
      "validation\n",
      "Epoch: 15 \tTraining Loss: 0.867195 \tTraining acc: 0.65 \tValidation Loss: 0.778254 \tAccuracy: 0.67 \tF1-Score: 0.65\n",
      "Validation loss decreased (0.804614 --> 0.778254).  Saving model ...\n",
      "validation\n",
      "Epoch: 16 \tTraining Loss: 0.853559 \tTraining acc: 0.64 \tValidation Loss: 0.793606 \tAccuracy: 0.70 \tF1-Score: 0.70\n",
      "validation\n",
      "Epoch: 17 \tTraining Loss: 0.827071 \tTraining acc: 0.64 \tValidation Loss: 0.864762 \tAccuracy: 0.65 \tF1-Score: 0.65\n",
      "validation\n",
      "Epoch: 18 \tTraining Loss: 0.810148 \tTraining acc: 0.67 \tValidation Loss: 0.725627 \tAccuracy: 0.71 \tF1-Score: 0.68\n",
      "Validation loss decreased (0.778254 --> 0.725627).  Saving model ...\n",
      "validation\n",
      "Epoch: 19 \tTraining Loss: 0.819458 \tTraining acc: 0.67 \tValidation Loss: 0.732953 \tAccuracy: 0.69 \tF1-Score: 0.68\n",
      "validation\n",
      "Epoch: 20 \tTraining Loss: 0.822718 \tTraining acc: 0.65 \tValidation Loss: 0.715362 \tAccuracy: 0.70 \tF1-Score: 0.69\n",
      "Validation loss decreased (0.725627 --> 0.715362).  Saving model ...\n",
      "validation\n",
      "Epoch: 21 \tTraining Loss: 0.804204 \tTraining acc: 0.67 \tValidation Loss: 0.752876 \tAccuracy: 0.69 \tF1-Score: 0.69\n",
      "validation\n",
      "Epoch: 22 \tTraining Loss: 0.799929 \tTraining acc: 0.66 \tValidation Loss: 0.735604 \tAccuracy: 0.70 \tF1-Score: 0.71\n",
      "validation\n",
      "Epoch: 23 \tTraining Loss: 0.785609 \tTraining acc: 0.68 \tValidation Loss: 0.745026 \tAccuracy: 0.72 \tF1-Score: 0.72\n",
      "validation\n",
      "Epoch: 24 \tTraining Loss: 0.778533 \tTraining acc: 0.67 \tValidation Loss: 0.684809 \tAccuracy: 0.73 \tF1-Score: 0.72\n",
      "Validation loss decreased (0.715362 --> 0.684809).  Saving model ...\n",
      "validation\n",
      "Epoch: 25 \tTraining Loss: 0.774161 \tTraining acc: 0.69 \tValidation Loss: 0.708775 \tAccuracy: 0.71 \tF1-Score: 0.70\n",
      "validation\n",
      "Epoch: 26 \tTraining Loss: 0.751396 \tTraining acc: 0.68 \tValidation Loss: 0.674574 \tAccuracy: 0.73 \tF1-Score: 0.73\n",
      "Validation loss decreased (0.684809 --> 0.674574).  Saving model ...\n",
      "validation\n",
      "Epoch: 27 \tTraining Loss: 0.764661 \tTraining acc: 0.69 \tValidation Loss: 0.706630 \tAccuracy: 0.71 \tF1-Score: 0.70\n",
      "validation\n",
      "Epoch: 28 \tTraining Loss: 0.737068 \tTraining acc: 0.70 \tValidation Loss: 0.766629 \tAccuracy: 0.69 \tF1-Score: 0.68\n",
      "validation\n",
      "Epoch: 29 \tTraining Loss: 0.754546 \tTraining acc: 0.68 \tValidation Loss: 0.685131 \tAccuracy: 0.74 \tF1-Score: 0.73\n",
      "validation\n",
      "Epoch: 30 \tTraining Loss: 0.734254 \tTraining acc: 0.69 \tValidation Loss: 0.708758 \tAccuracy: 0.72 \tF1-Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_epochs= 30\n",
    "train_on_gpu = True\n",
    "size = 112\n",
    "\n",
    "eval_preprocess = eval_preprocess(size)\n",
    "\n",
    "train_preprocess = train_preprocess(size)\n",
    "\n",
    "train_dataset = VideoFrameDataset(\n",
    "    root_path='transfer/train',\n",
    "    annotationfile_path='transfer/train/annotations.txt',\n",
    "    num_segments=16,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=train_preprocess,\n",
    "    test_mode=False\n",
    ")\n",
    "test_dataset = VideoFrameDataset(\n",
    "    root_path='transfer/test',\n",
    "    annotationfile_path='transfer/test/annotations.txt',\n",
    "    num_segments=1,\n",
    "    frames_per_segment=16,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=eval_preprocess,\n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,#2\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,#2\n",
    "    pin_memory=True\n",
    ")\n",
    "'''\n",
    "model = MobileNetV2(num_classes=6, sample_size=size, width_mult=1.)\n",
    "#model.load_state_dict(torch.load('pretrained/kinetics_mobilenetv2_1.0x_RGB_16_best.pth'))'''\n",
    "\n",
    "if train_on_gpu:\n",
    "    model = nn.DataParallel(model)\n",
    "    model.cuda()\n",
    "    print('train on gpu')\n",
    "else:\n",
    "    model.cpu()\n",
    "    print('train on cpu')\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min',patience=10,verbose=True,min_lr=1e-10)\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "epoch_train_loss =[]\n",
    "epoch_val_loss =[]\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    train_pred = np.array([],dtype='i')\n",
    "    train_truth = np.array([],dtype='i')\n",
    "    for data, target in train_dataloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        train_pred = np.concatenate((train_pred, np.argmax(output.clone().detach().cpu().numpy(),axis=1)))\n",
    "        train_truth = np.concatenate((train_truth, target.clone().detach().cpu().numpy()))\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    y_pred = np.array([],dtype='i')\n",
    "    y_truth = np.array([],dtype='i')\n",
    "    print('validation')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            y_pred = np.concatenate((y_pred, np.argmax(output.clone().detach().cpu().numpy(),axis=1)))\n",
    "            y_truth = np.concatenate((y_truth, target.clone().detach().cpu().numpy()))\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_dataloader.sampler)\n",
    "    valid_loss = valid_loss/len(test_dataloader.sampler)\n",
    "    epoch_train_loss.append(train_loss)\n",
    "    epoch_val_loss.append(valid_loss)\n",
    "    \n",
    "    trainacc=accuracy_score(train_truth, train_pred)\n",
    "    acc = accuracy_score(y_truth, y_pred)\n",
    "    rec = recall_score(y_truth, y_pred, average='macro')\n",
    "    prec = precision_score(y_truth, y_pred, average='macro')\n",
    "    f1 = f1_score(y_truth, y_pred, average='macro')\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining acc: {:.2f} \\tValidation Loss: {:.6f} \\tAccuracy: {:.2f} \\tF1-Score: {:.2f}'.format(\n",
    "    epoch, train_loss, trainacc, valid_loss, acc, f1))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.module.state_dict(), 'P3D63_Global_2.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "    \"\"\"\n",
    "    wandb.log({\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': valid_loss,\n",
    "        'accuracy': acc,\n",
    "        'f1-score': f1,\n",
    "        'recall': rec,\n",
    "        'precission': prec,\n",
    "    }, step=epoch) \n",
    "    \"\"\"\n",
    "    \n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "11.425484초\n"
     ]
    }
   ],
   "source": [
    "test_dataset = VideoFrameDataset(\n",
    "    root_path='test/finalfull/test',\n",
    "    annotationfile_path='test/finalfull/test/annotations.txt',\n",
    "    num_segments=1,\n",
    "    frames_per_segment=16,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=eval_preprocess,\n",
    "    test_mode=False\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,#2\n",
    "    pin_memory=True\n",
    ")\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "model.eval()\n",
    "y_pred = np.array([],dtype='i')\n",
    "print('validation')\n",
    "with torch.no_grad():\n",
    "    for data, target in test_dataloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        y_pred = np.concatenate((y_pred, np.argmax(output.clone().detach().cpu().numpy(),axis=1)))\n",
    "terminate_time = timeit.default_timer()\n",
    "print(\"%f초\" % (terminate_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 5 0 5 0 5 5 5 5 1 5 0 5 1 1 5 5 1 5 1 0 5 1 1 5 5 1 5 5 1 3 1 5\n",
      " 0 1 0 1 5 0 0 0 5 5 5 5 1 1 5 5 3 5 0 5 0 3 5 5 0 5 1 5 5 5 1 1 5 5 1 1 0\n",
      " 5 0 5 0 0 5 5 0 0 1 1 5 0 3 0 1 1 0 1 1 1 0 5 5 1 1 1 0 1 1 5 1 5 1 0 0 0\n",
      " 0 1 1 5 1 5 5 5 0 5 1 0 5 0 0 5 0 0 1 0 5 1 1 5 5 5 0 0 0 1 0 1 1 1 1 5 0\n",
      " 1 5 1 1 5 0 1 5 5 3 5 0 1 0 1 1 0 0 5 5 5 5 1 0 5 1 5 1 0 0 0 5 1 1 1 0 5\n",
      " 1 0 0 1 1 0 5 5 5 0 5 0 3 5 0 1 0 1 1 1 1 5 1 1 1 1 0 0 5 5 5 0 0 1 1 5 1\n",
      " 5 5 0 1 5 1 5 5 5 3 1 0 5 0 1 5 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 5 1 1 1 1 2 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 5 1 1 1 5 1 1 5 1 1 1 1 1 5 1 1 1 1\n",
      " 1 1 1 1 1 1 5 1 1 1 1 1 1 1 5 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 5 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5\n",
      " 1 5 5 5 5 5 5 0 5 1 1 5 5 5 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 5 5 5 5 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as numpy\n",
    "y_truth=numpy.loadtxt('final/label.txt',dtype='int', delimiter=\"\\n\")\n",
    "print(y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3237704918032787"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_truth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x=y_pred\n",
    "i=0\n",
    "for a in y_pred:\n",
    "    if a == 1 or a == 5:\n",
    "        print(a)\n",
    "        x[i]=1\n",
    "    else :\n",
    "        x[i]=0\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0\n",
      " 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1\n",
      " 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y_truth\n",
    "i=0\n",
    "for a in y_truth:\n",
    "    if a == 1 or a == 5:\n",
    "        y[i]=True\n",
    "    else :\n",
    "        y[i]=False\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6311475409836066"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(x, y) #1과 5를 같은 행동이라고 생각했을때 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "3\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "3\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x=y_pred\n",
    "i=0\n",
    "for a in y_pred:\n",
    "    if a == 1 or a==3 or a == 5:\n",
    "        x[i]=1\n",
    "    else :\n",
    "        x[i]=0\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1\n",
      " 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1\n",
      " 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1\n",
      " 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y_truth\n",
    "i=0\n",
    "for a in y_truth:\n",
    "    if a == 1 or a==3 or a == 5:\n",
    "        y[i]=True\n",
    "    else :\n",
    "        y[i]=False\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6516393442622951"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(x, y) #집중 비집중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
